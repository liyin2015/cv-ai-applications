
# Vision Language Models (VL)

## Self-supervised learning
SOTA paper:
* [Baevski, Alexei, et al. "Data2vec: A general framework for self-supervised learning in speech, vision and language." URL https://ai. facebook. com/research/data2veca-general-framework-for-self-supervi sed-learning-in-speech-vision-and-la nguage/. Accessed (2022): 01-27.](http://transformers.science/rct/articles/Baevski%20et%20al.-2022-data2vec%20A%20General%20Framework%20for%20Self-supervised%20Learning%20in%20Speech%2C%20Vision%20and%20Language.pdf). From this paper, we can find landmark papers for vision self-supervised learning. Data2vec predicts the latent representations of the
input data.

Landmark papers:

* CLIP: [Radford, Alec, et al. "Learning transferable visual models from natural language supervision." International Conference on Machine Learning. PMLR, 2021.](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf). #cite: 500. Contrastive Language-Image Pre-training.


## Other resources
* [awesome-vision-and-language](https://github.com/sangminwoo/awesome-vision-and-language)